# RAG Starter Repo

A **production-ready starter template** for building Generative AI applications ‚Äî chat systems, RAG pipelines, multi-model orchestration, and inference services ‚Äî **without hardcoding logic or coupling your app to a single LLM provider**.

This repo gives you a clean separation between:

* models
* prompts
* retrieval
* preprocessing
* inference

So your system doesn‚Äôt collapse the moment requirements change.

---

## Why this exists

Most ‚ÄúGenAI repos‚Äù are:

* notebooks pretending to be systems
* tightly coupled to one model
* impossible to extend without rewrites

This repo is different.

**Design goals:**

* Model-agnostic (OpenAI, Anthropic, local, future models)
* RAG-first but optional
* Clean abstractions (no LangChain lock-in)
* Easy to deploy, test, and scale
* Works for real products, not demos

---

## Project Structure

```
generative_ai_project/
‚îú‚îÄ‚îÄ config/                 # Model & logging configs
‚îÇ   ‚îú‚îÄ‚îÄ model_config.yaml
‚îÇ   ‚îî‚îÄ‚îÄ logging_config.yaml
‚îÇ
‚îú‚îÄ‚îÄ data/                   # Local runtime data
‚îÇ   ‚îú‚îÄ‚îÄ cache/              # Cached responses
‚îÇ   ‚îú‚îÄ‚îÄ embeddings/         # Generated embeddings
‚îÇ   ‚îî‚îÄ‚îÄ vectordb/           # Vector DB indexes (FAISS / Chroma)
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ core/               # LLM abstraction layer
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_llm.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gpt_client.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ claude_client.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ local_llm.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ model_factory.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ prompts/            # Prompt templates & chains
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ templates.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ chain.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ rag/                # Retrieval-Augmented Generation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embedder.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ retriever.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vector_store.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ indexer.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ processing/         # Text preprocessing
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chunking.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tokenizer.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ preprocessor.py
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ inference/          # Inference orchestration
‚îÇ       ‚îú‚îÄ‚îÄ inference_engine.py
‚îÇ       ‚îî‚îÄ‚îÄ response_parser.py
‚îÇ
‚îú‚îÄ‚îÄ docs/                   # Documentation
‚îÇ   ‚îú‚îÄ‚îÄ README.md
‚îÇ   ‚îî‚îÄ‚îÄ SETUP.md
‚îÇ
‚îú‚îÄ‚îÄ scripts/                # Automation scripts
‚îÇ   ‚îú‚îÄ‚îÄ setup_env.sh
‚îÇ   ‚îú‚îÄ‚îÄ run_tests.sh
‚îÇ   ‚îú‚îÄ‚îÄ build_embeddings.py
‚îÇ   ‚îî‚îÄ‚îÄ cleanup.py
‚îÇ
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ .gitignore
```

---

## Core Concepts

### 1. Model Abstraction (No Vendor Lock-in)

All models implement a shared interface:

```python
class BaseLLM:
    def generate(self, prompt: str) -> str:
        ...
```

Switch providers via config ‚Äî not code.

---

### 2. Prompt Engineering Is a First-Class Citizen

Prompts live in `src/prompts/`, not scattered across files.

* Reusable templates
* Multi-step chains
* Versionable logic

No magic strings buried in handlers.

---

### 3. RAG Without Framework Hell

RAG is split into:

* Embedding
* Indexing
* Retrieval
* Generation

Each component is replaceable.

Use FAISS today, Chroma tomorrow, or your own store.

---

### 4. Deterministic Preprocessing

Text cleaning, chunking, and tokenization are **explicit**.

This matters for:

* reproducibility
* evals
* debugging hallucinations

---

### 5. Inference as an Engine

Inference is orchestrated, not improvised.

This is where you add:

* guardrails
* retries
* fallbacks
* structured outputs

---

## Quick Start

### 1. Clone

```bash
git clone <repo-url>
cd generative_ai_project
```

### 2. Create virtual environment

```bash
python -m venv .venv
source .venv/bin/activate
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Configure models

Edit:

```
config/model_config.yaml
```

Example:

```yaml
provider: openai
model: gpt-4o-mini
temperature: 0.2
```

---

## Example Flow

```text
Input
  ‚Üì
Preprocessing
  ‚Üì
( Optional ) Retrieval
  ‚Üì
Prompt Chain
  ‚Üì
LLM Generation
  ‚Üì
Response Parsing
  ‚Üì
Output
```

---

## Common Use Cases

* Chatbots with memory
* RAG over PDFs / docs
* Internal copilots
* Multi-model routing
* AI APIs (FastAPI / Flask)
* Agent-based systems

---

## What this repo is NOT

* ‚ùå A LangChain wrapper
* ‚ùå A tutorial notebook
* ‚ùå A one-off demo
* ‚ùå A ‚Äújust add prompts‚Äù repo

This is a **foundation**, not a toy.

---

## Extending the Repo

You can easily add:

* FastAPI service layer
* Agent frameworks
* Tool calling
* Evaluation pipelines
* Observability (Langfuse, OpenTelemetry)
* Multi-tenant memory

The structure won‚Äôt fight you.

---

## Who should use this

* Engineers building GenAI products
* Hackathon teams shipping fast
* Founders prototyping real systems
* Anyone tired of rewriting GenAI code every month

---

## License

MIT ‚Äî do whatever you want, just don‚Äôt pretend you wrote it from scratch üòâ


